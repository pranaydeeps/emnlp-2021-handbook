SubmissionNumber#=%=#43
FinalPaperTitle#=%=#Simple and Efficient ways to Improve REALM
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Dense retrieval has been shown to be effective for Open Domain Question Answering, surpassing sparse retrieval methods like BM25. 
One such model, REALM, (Guu et al., 2020) is an end-to-end 
dense retrieval system that uses MLM based pretraining for improved downstream QA
performance. However, the current REALM setup 
uses limited resources and is not comparable in scale to more recent systems, contributing to its lower performance.
Additionally, it relies on noisy supervision for retrieval during fine-tuning.
We propose REALM++, where we improve upon the training and inference setups and introduce better supervision signal for improving performance, without any architectural changes.
REALM++ achieves ~5.5% absolute accuracy gains over the baseline while being faster to train. It also matches the performance of large models which have 3x more parameters demonstrating the efficiency of our setup.
Author{1}{Firstname}#=%=#Vidhisha
Author{1}{Lastname}#=%=#Balachandran
Author{1}{Username}#=%=#vidhisha
Author{1}{Email}#=%=#vbalacha@cs.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Ashish
Author{2}{Lastname}#=%=#Vaswani
Author{2}{Username}#=%=#avaswani
Author{2}{Email}#=%=#avaswani@google.com
Author{2}{Affiliation}#=%=#Google Research
Author{3}{Firstname}#=%=#Yulia
Author{3}{Lastname}#=%=#Tsvetkov
Author{3}{Username}#=%=#yulia.tsvetkov
Author{3}{Email}#=%=#yuliats@cs.washington.edu
Author{3}{Affiliation}#=%=#University of Washington
Author{4}{Firstname}#=%=#Niki
Author{4}{Lastname}#=%=#Parmar
Author{4}{Username}#=%=#nikip
Author{4}{Email}#=%=#nikiparmar09@gmail.com
Author{4}{Affiliation}#=%=#Google

==========