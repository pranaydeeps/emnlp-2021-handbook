* Wednesday, November 10, 2021
4   # Low Resource Quadratic Forms for Knowledge Graph Embeddings
9   # Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools
12   # Limitations of Knowledge Distillation for Zero-shot Transfer Learning
13   # Countering the Influence of Essay Length in Neural Essay Scoring
14   # Memory-efficient Transformers via Top-k Attention
17   # BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models
19   # Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering
20   # Learning to Rank in the Age of Muppets: Effectiveness___Efficiency Tradeoffs in Multi-Stage Ranking
24   # Improving Synonym Recommendation Using Sentence Context
27   # Semantic Categorization of Social Knowledge for Commonsense Question Answering
36   # {S}peeding Up {T}ransformer Training By Using Dataset Subsampling - {A}n Exploratory Analysis
39   # Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search
40   # Hyperparameter Power Impact in Transformer Language Model Training
41   # Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing
43   # Simple and Efficient ways to Improve REALM
44   # Shrinking Bigfoot: Reducing wav2vec 2.0 footprint
47   # On the Role of Corpus Ordering in Language Modeling
49   # Efficient Domain Adaptation of Language Models via Adaptive Tokenization
52   # Unsupervised Contextualized Document Representation
53   # Logistic Regression Trained on Learner Data Outperformed Neural Language Models in Unsupervised Automatic Readability Assessment
