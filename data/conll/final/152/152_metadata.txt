SubmissionNumber#=%=#152
FinalPaperTitle#=%=#Understanding the Extent to which Content Quality Metrics Measure the Information Quality of Summaries
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Daniel Deutsch
JobTitle#==#
Organization#==#
Abstract#==#Reference-based metrics such as ROUGE or BERTScore evaluate the content quality of a summary by comparing the summary to a reference. Ideally, this comparison should measure the summary's information quality by calculating how much information the summaries have in common. In this work, we analyze the token alignments used by ROUGE and BERTScore to compare summaries and argue that their scores largely cannot be interpreted as measuring information overlap. Rather, they are better estimates of the extent to which the summaries discuss the same topics. Further, we provide evidence that this result holds true for many other summarization evaluation metrics. The consequence of this result is that the most frequently used summarization evaluation metrics do not align with the community's research goal, to generate summaries with high-quality information. However, we conclude by demonstrating that a recently proposed metric, QAEval, which scores summaries using question-answering, appears to better capture information quality than current evaluations, highlighting a direction for future research.
Author{1}{Firstname}#=%=#Daniel
Author{1}{Lastname}#=%=#Deutsch
Author{1}{Username}#=%=#danieldeutsch
Author{1}{Email}#=%=#ddeutsch@seas.upenn.edu
Author{1}{Affiliation}#=%=#University of Pennsylvania
Author{2}{Firstname}#=%=#Dan
Author{2}{Lastname}#=%=#Roth
Author{2}{Username}#=%=#danr
Author{2}{Email}#=%=#danroth@seas.upenn.edu
Author{2}{Affiliation}#=%=#University of Pennsylvania

==========