SubmissionNumber#=%=#4
FinalPaperTitle#=%=#Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words
ShortPaperTitle#=%=#Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words
NumberOfPages#=%=#9
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#We present an unsupervised topic model for short texts that performs soft
clustering over distributed representations of words. We model the
low-dimensional semantic vector space represented by the dense distributed
representations of words using Gaussian mixture models (GMMs) whose components
capture the notion of latent topics. While conventional topic modeling schemes
such as probabilistic latent semantic analysis (pLSA) and latent Dirichlet
allocation (LDA) need aggregation of short messages to avoid data sparsity in
short documents, our framework works on large amounts of raw short texts
(billions of words). In contrast with other topic modeling frameworks that use
word co-occurrence statistics, our framework uses a vector space model that
overcomes the issue of sparse word co-occurrence patterns. We demonstrate that
our framework outperforms LDA on short texts through both subjective and
objective evaluation. We also show the utility of our framework in learning
topics and classifying short texts on Twitter data for English, Spanish,
French, Portuguese and Russian.
Author{1}{Firstname}#=%=#Vivek Kumar
Author{1}{Lastname}#=%=#Rangarajan Sridhar
Author{1}{Email}#=%=#vivek_136@yahoo.com
Author{1}{Affiliation}#=%=#AT&T Labs - Research

==========