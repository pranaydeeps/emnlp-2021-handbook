SubmissionNumber#=%=#13
FinalPaperTitle#=%=#Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Emmanuele Chersoni
JobTitle#==#
Organization#==#Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University, Yuk Choi Road 11, Hung Hom, Kowloon, Hong Kong
Abstract#==#With the recent rise in popularity of Transformer models in Natural Language Processing, research efforts have been dedicated to the development of domain-adapted versions of BERT-like architectures. In this study, we focus on FinBERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pretraining from scratch, conversely, seems to be less effective.
Author{1}{Firstname}#=%=#Bo
Author{1}{Lastname}#=%=#Peng
Author{1}{Username}#=%=#bo_peng
Author{1}{Email}#=%=#peng-bo.peng@polyu.edu.hk
Author{1}{Affiliation}#=%=#The Hong Kong Polytechnic University
Author{2}{Firstname}#=%=#Emmanuele
Author{2}{Lastname}#=%=#Chersoni
Author{2}{Username}#=%=#manuandoscar
Author{2}{Email}#=%=#emmanuelechersoni@gmail.com
Author{2}{Affiliation}#=%=#Hong Kong Polytechnic University
Author{3}{Firstname}#=%=#Yu-Yin
Author{3}{Lastname}#=%=#Hsu
Author{3}{Username}#=%=#yuyin
Author{3}{Email}#=%=#yyhsu@polyu.edu.hk
Author{3}{Affiliation}#=%=#The Hong Kong Polytechnic University
Author{4}{Firstname}#=%=#Chu-Ren
Author{4}{Lastname}#=%=#Huang
Author{4}{Username}#=%=#churen.huang
Author{4}{Email}#=%=#churen.huang@polyu.edu.hk
Author{4}{Affiliation}#=%=#The Hong Kong Polytechnic Universiy

==========