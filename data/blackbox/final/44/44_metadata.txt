SubmissionNumber#=%=#44
FinalPaperTitle#=%=#Multi-Layer Random Perturbation Training for improving Model Generalization Efficiently
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Lis Weiji Kanashiro Pereira
JobTitle#==#
Organization#==#Ochanomizu University
Abstract#==#We propose a simple yet effective Multi-Layer RAndom Perturbation Training algorithm (RAPT) to enhance model robustness and generalization. The key idea is to apply randomly sampled noise to each input to generate label-preserving artificial input points. To encourage the model to generate more diverse examples, the noise is added to a combination of the model layers. Then, our model regularizes the posterior difference between clean and noisy inputs.
We apply RAPT  towards robust and efficient BERT training, and conduct comprehensive fine-tuning experiments on GLUE tasks. Our results show that RAPT outperforms the standard fine-tuning approach, and adversarial training method, yet with 22% less training time.
Author{1}{Firstname}#=%=#Lis
Author{1}{Lastname}#=%=#Kanashiro Pereira
Author{1}{Username}#=%=#liskp
Author{1}{Email}#=%=#liskanashiro@gmail.com
Author{1}{Affiliation}#=%=#Ochanomizu University
Author{2}{Firstname}#=%=#Yuki
Author{2}{Lastname}#=%=#Taya
Author{2}{Username}#=%=#yukitaya
Author{2}{Email}#=%=#yuki.530.taya@gmail.com
Author{2}{Affiliation}#=%=#Ochanomizu University
Author{3}{Firstname}#=%=#Ichiro
Author{3}{Lastname}#=%=#Kobayashi
Author{3}{Username}#=%=#koba
Author{3}{Email}#=%=#koba@is.ocha.ac.jp
Author{3}{Affiliation}#=%=#Ochanomizu University

==========