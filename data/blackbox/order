* Thursday, November 11, 2021
= Virtual Programme
+ 2:00--2:15 Opening Remarks
! 2:15--3:00 Invited Talk by Jelle Zuidema \& Q\&A
! 3:15--4:00 Oral Session 1
! 4:30--6:00 Poster Session 1
! 6:15--7:00 Oral Session 2
! 7:30--8:00 Invited Talk by Ana Marasović
! 8:00--8:30 Invited Talk by Sara Hooker
+ 8:30--8:45 Closing
= Hybrid Programme
+ 9:00--9:15 Opening Remarks \& Best Paper Award
! 9:15--10:00 Invited Talk by Jelle Zuidema \& Q\&A
! 10:00--10:30 Oral Session 3
! 11:00--12:00 Poster Session 2
! 13:00--13:45 Invited Talk by Sara Hooker \& Q\&A
! 13:45--14:15 Oral Session 4
! 14:45--16:15 Poster Session 3
! 16:45--17:15 Oral Session 5
! 17:15--18:00 Invited Talk by Ana Marasović \& Q\&A
+ 18:00--18:15 Closing
= Oral Sessions
56 # To what extent do human explanations of model behavior align with actual model behavior?
65 # Test Harder than You Train: Probing with Extrapolation Splits
13 # Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings
24 # The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation
83 # On the Limits of Minimal Pairs in Contrastive Evaluation
87 # What Models Know About Their Attackers: Deriving Attacker Information From Latent Representations
= Poster Sessions
2 # ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns___ Semantic Properties and their Prototypicality
5 # ProSPer: Probing Human and Neural Network Language Model Understanding of Spatial Perspective
6 # Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN
7 # Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?
10 # Discrete representations in neural models of spoken language
12 # Word Equations: Inherently Interpretable Sparse Word Embeddings through Sparse Coding
14 # A howling success or a working sea? Testing what BERT knows about metaphors
15 # How Length Prediction Influence the Performance of Non-Autoregressive Translation?
18 # On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning
22 # Relating Neural Text Degeneration to Exposure Bias
23 # Efficient Explanations from Empirical Explainers
30 # Variation and generality in encoding of syntactic anomaly information in sentence embeddings
32 # Enhancing Interpretable Clauses Semantically using Pretrained Word Representation
39 # Analyzing BERT's Knowledge of Hypernymy via Prompting
41 # An in-depth look at Euclidean disk embeddings for structure preserving parsing
42 # Training Dynamic based data filtering may not work for NLP datasets
44 # Multi-Layer Random Perturbation Training for improving Model Generalization Efficiently
51 # Screening Gender Transfer in Neural Machine Translation
52 # What BERT Based Language Model Learns in Spoken Transcripts: An Empirical Study
60 # Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference
63 # Investigating Negation in Pre-trained Vision-and-language Models
66 # Not all parameters are born equal: Attention is mostly what you need
69 # Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids___ Representations
71 # Learning Mathematical Properties of Integers
74 # Probing Language Models for Understanding of Temporal Expressions
75 # How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings
77 # Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing
78 # An Investigation of Language Model Interpretability via Sentence Editing
79 # Interacting Knowledge Sources, Inspection and Analysis: Case-studies on Biomedical text processing
81 # Attacks against Ranking Algorithms with Text Embeddings: {A} Case Study on Recruitment Algorithms
82 # Controlled tasks for model analysis: Retrieving discrete information from sequences
84 # The Acceptability Delta Criterion: Testing Knowledge of Language using the Gradience of Sentence Acceptability
85 # How Does BERT Rerank Passages? An Attribution Analysis with Information Bottlenecks
92 # Do Language Models Know the Way to Rome?
98 # Exploratory Model Analysis Using Data-Driven Neuron Representations
99 # Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers
100 # BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology

= Non-archival papers (posters)
! Language Models Use Monotonicity to Assess NPI Licensing -- Jaap Jumelet, Milica Denic, Jakub Szymanik, Dieuwke Hupkes and Shane Steinert-Threlkeld
! BPE affects Training Data Memorization by Transformer Language Models -- Eugene Kharitonov, Marco Baroni and Dieuwke Hupkes
! Do contextual language embeddings distinguish between intersective and strictly subsective adjectives? -- Michael Goodale and Salvador Mascarenhas
! Generalization in neural sequence models: a case study in symbolic mathematics -- Sean Welleck, Peter West, Jize Cao and Yejin Choi
! Human Evaluation Study for Explaining Knowledge Graph Completion -- Timo Sztyler and Carolin Lawrence
! Transformers Scan both Left and Right -- When they Have a Cue -- Jan H. Athmer and Denis Paperno
! Explaining NLP Models via Minimal Contrastive Editing (MiCE) -- Alexis Ross, Ana Marasović and Matthew Peters
! Probing structures in the visual region embeddings from multimodal BERT -- Victor Milewski, Miryam de Lhoneux and Marie-Francine Moens
! Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords -- Taelin Karidi, Yichu Zhou, Nathan Schneider, Omri Abend and Vivek Srikumar
! On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation -- Gal Patel, Leshem Choshen and Omri Abend
! Explaining Classes through Word Attributions -- Samuel Rönnqvist, Amanda Myntti, Aki-Juhani Kyröläinen, Sampo Pyysalo, Veronika Laippala and Filip Ginter
! Testing the linguistics of transformer generalizations -- Saliha Muradoglu and Mans Hulden