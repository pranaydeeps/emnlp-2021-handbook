SubmissionNumber#=%=#57
FinalPaperTitle#=%=#Investigating Pretrained Language Models for Graph-to-Text Generation
ShortPaperTitle#=%=#
NumberOfPages#=%=#17
CopyrightSigned#=%=#Leonardo Ribeiro
JobTitle#==#
Organization#==#Technische Universität Darmstadt, Karolinenpl. 5, 64289 Darmstadt, Germany
Abstract#==#Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references.  In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.
Author{1}{Firstname}#=%=#Leonardo F. R.
Author{1}{Lastname}#=%=#Ribeiro
Author{1}{Username}#=%=#leonardoribeiro
Author{1}{Email}#=%=#ribeiro@aiphes.tu-darmstadt.de
Author{1}{Affiliation}#=%=#TU Darmstadt
Author{2}{Firstname}#=%=#Martin
Author{2}{Lastname}#=%=#Schmitt
Author{2}{Username}#=%=#mnschmit
Author{2}{Email}#=%=#martin@cis.lmu.de
Author{2}{Affiliation}#=%=#Center for Information and Language Processing, LMU Munich
Author{3}{Firstname}#=%=#Hinrich
Author{3}{Lastname}#=%=#Schütze
Author{3}{Username}#=%=#cislmu
Author{3}{Email}#=%=#inquiries@cislmu.org
Author{3}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich
Author{4}{Firstname}#=%=#Iryna
Author{4}{Lastname}#=%=#Gurevych
Author{4}{Username}#=%=#gurevych
Author{4}{Email}#=%=#gurevych@ukp.informatik.tu-darmstadt.de
Author{4}{Affiliation}#=%=#UKP Lab, Technische Universität Darmstadt

==========