SubmissionNumber#=%=#3461
FinalPaperTitle#=%=#Revisiting Robust Neural Machine Translation: A Transformer Case Study
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Puneeth Srinivas Mohan Saladi
JobTitle#==#
Organization#==#
Abstract#==#Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we introduce a novel data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two other novel extensions to the original Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that modify the neural architecture as well as the training process to handle noise. One important characteristic of our techniques is that they only impact the training phase and do not impose any overhead at inference time. We evaluated our techniques to translate the English--German pair in both directions and observed that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise.
Author{1}{Firstname}#=%=#Peyman
Author{1}{Lastname}#=%=#Passban
Author{1}{Username}#=%=#pe.psbn
Author{1}{Email}#=%=#pe.psbn@gmail.com
Author{1}{Affiliation}#=%=#Amazon
Author{2}{Firstname}#=%=#Puneeth
Author{2}{Lastname}#=%=#Saladi
Author{2}{Username}#=%=#puneethsaladi
Author{2}{Email}#=%=#puneeth.saladi@huawei.com
Author{2}{Affiliation}#=%=#Huawei Noah's Ark Lab
Author{3}{Firstname}#=%=#Qun
Author{3}{Lastname}#=%=#Liu
Author{3}{Username}#=%=#liuqun
Author{3}{Email}#=%=#qun.liu@huawei.com
Author{3}{Affiliation}#=%=#Huawei Noah's Ark Lab

==========