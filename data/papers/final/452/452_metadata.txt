SubmissionNumber#=%=#452
FinalPaperTitle#=%=#Penalized Expectation Propagation for Graphical Models over Strings
ShortPaperTitle#=%=#Penalized Expectation Propagation for Graphical Models over Strings
NumberOfPages#=%=#11
CopyrightSigned#=%=#Jason M. Eisner
JobTitle#==#
Organization#==#Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
USA
Abstract#==#We present penalized expectation propagation (PEP), a novel algorithm for
approximate inference in graphical models.  Expectation propagation is a
variant of loopy belief propagation that keeps messages tractable by projecting
them back into a given family of functions.  Our extension, PEP, uses a
structured-sparsity penalty to encourage simple messages, thus balancing speed
and accuracy.  We specifically show how to instantiate PEP in the case of
string-valued random variables, where we adaptively approximate finite-state
distributions by variable-order n-gram models.                                       
     

On
phonological
inference
problems, we obtain substantial speedup over previous related algorithms with
no significant loss in accuracy.
Author{1}{Firstname}#=%=#Ryan
Author{1}{Lastname}#=%=#Cotterell
Author{1}{Email}#=%=#ryan.cotterell@gmail.com
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#Jason
Author{2}{Lastname}#=%=#Eisner
Author{2}{Email}#=%=#jason@cs.jhu.edu
Author{2}{Affiliation}#=%=#Johns Hopkins University

==========