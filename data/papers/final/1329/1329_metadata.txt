SubmissionNumber#=%=#1329
FinalPaperTitle#=%=#Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Shaolei Zhang
JobTitle#==#
Organization#==#Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)
University of Chinese Academy of Sciences, Beijing, China
Abstract#==#Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central words and then spreads around them. In this work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation.
Author{1}{Firstname}#=%=#Shaolei
Author{1}{Lastname}#=%=#Zhang
Author{1}{Username}#=%=#zhangshaolei
Author{1}{Email}#=%=#zhangshaolei20z@ict.ac.cn
Author{1}{Affiliation}#=%=#Institute of Computing Technology, Chinese Academy of Sciences
Author{2}{Firstname}#=%=#Yang
Author{2}{Lastname}#=%=#Feng
Author{2}{Username}#=%=#y.feng
Author{2}{Email}#=%=#fengyang@ict.ac.cn
Author{2}{Affiliation}#=%=#Institute of Computing Technology, Chinese Academy of Sciences

==========