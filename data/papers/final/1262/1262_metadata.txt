SubmissionNumber#=%=#1262
FinalPaperTitle#=%=#Continuous Entailment Patterns for Lexical Inference in Context
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Martin Schmitt
JobTitle#==#PhD candidate
Organization#==#Ludwig-Maximilians-Universität München
Centrum für Informations- und Sprachverarbeitung
Oettingenstraße 67
D-80538 München
Abstract#==#Combining a pretrained language model (PLM) with textual patterns has been shown to help
in both zero- and few-shot settings.
For zero-shot performance, it makes sense to design patterns
that closely resemble the text seen during self-supervised pretraining
because the model has never seen anything else.
Supervised training allows for more flexibility.
If we allow for tokens outside the PLM's vocabulary,
patterns can be adapted more flexibly to a PLM's idiosyncrasies.
Contrasting patterns
where a "token" can be any continuous vector from those
where a discrete choice between vocabulary elements has to be made,
we call our method CONtinous pAtterNs (CONAN).
We evaluate CONAN on two established benchmarks
for lexical inference in context (LIiC) a.k.a. predicate entailment,
a challenging natural language understanding task with relatively small training data.
In a direct comparison with discrete patterns,
CONAN consistently leads to improved performance,
setting a new state of the art.
Our experiments give valuable insights on the kind of pattern that enhances a PLM's performance on LIiC
and raise important questions regarding our understanding of PLMs using text patterns.
Author{1}{Firstname}#=%=#Martin
Author{1}{Lastname}#=%=#Schmitt
Author{1}{Username}#=%=#mnschmit
Author{1}{Email}#=%=#martin@cis.lmu.de
Author{1}{Affiliation}#=%=#Center for Information and Language Processing, LMU Munich
Author{2}{Firstname}#=%=#Hinrich
Author{2}{Lastname}#=%=#Schütze
Author{2}{Username}#=%=#cislmu
Author{2}{Email}#=%=#inquiries@cislmu.org
Author{2}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich

==========