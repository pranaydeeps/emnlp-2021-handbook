SubmissionNumber#=%=#3670
FinalPaperTitle#=%=#How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Hoang Van
JobTitle#==#
Organization#==#Department of Computer Science, the University of Arizona, Tucson, AZ, USA
Abstract#==#The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper,  we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways:   simplifying input texts at prediction time and augmenting data to provide machines with additional information during training.   We demonstrate that the latter scenario provides positive effects on machine performance on two separate datasets.  In particular,  the latter use of  TS  improves the performances of LSTM (1.82–1.98%) and SpanBERT  (0.7–1.3%)  extractors on  TACRED,  a complex, large-scale, real-world relation extraction task. Further, the same setting yields improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT text classifier on MNLI,  a practical natural language inference dataset.
Author{1}{Firstname}#=%=#Hoang
Author{1}{Lastname}#=%=#Van
Author{1}{Username}#=%=#vnhh
Author{1}{Email}#=%=#vnhh@email.arizona.edu
Author{1}{Affiliation}#=%=#University of Arizona
Author{2}{Firstname}#=%=#Zheng
Author{2}{Lastname}#=%=#Tang
Author{2}{Username}#=%=#zhengtang
Author{2}{Email}#=%=#zhengtang@email.arizona.edu
Author{2}{Affiliation}#=%=#University of Arizona
Author{3}{Firstname}#=%=#Mihai
Author{3}{Lastname}#=%=#Surdeanu
Author{3}{Username}#=%=#surdeanu
Author{3}{Email}#=%=#surdeanu@gmail.com
Author{3}{Affiliation}#=%=#University of Arizona

==========