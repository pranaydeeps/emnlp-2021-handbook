SubmissionNumber#=%=#3308
FinalPaperTitle#=%=#Discrete and Soft Prompting for Multilingual Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#9
CopyrightSigned#=%=#Mengjie Zhao
JobTitle#==#
Organization#==#
Abstract#==#It has been shown for English that discrete and soft prompting  perform strongly
in few-shot learning with pretrained language models (PLMs).
In this paper, we show that discrete and soft prompting perform better
than
finetuning in multilingual cases:
Crosslingual transfer
and
in-language training of
multilingual natural language inference.
For example, with 48 English training examples, finetuning obtains
33.74\% accuracy in crosslingual transfer, barely surpassing the
majority baseline (33.33\%).
In contrast, discrete and soft
prompting outperform finetuning, achieving 36.43\% and 38.79\%.
We also
demonstrate good performance of prompting
with training data in multiple languages other than English.
Author{1}{Firstname}#=%=#Mengjie
Author{1}{Lastname}#=%=#Zhao
Author{1}{Username}#=%=#mzhao
Author{1}{Email}#=%=#mengjie.zhao@cis.lmu.de
Author{1}{Affiliation}#=%=#Center for Information and Language Processing, LMU Munich
Author{2}{Firstname}#=%=#Hinrich
Author{2}{Lastname}#=%=#Sch√ºtze
Author{2}{Username}#=%=#cislmu
Author{2}{Email}#=%=#inquiries@cislmu.org
Author{2}{Affiliation}#=%=#Center for Information and Language Processing, University of Munich

==========