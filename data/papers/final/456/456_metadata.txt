SubmissionNumber#=%=#456
FinalPaperTitle#=%=#Beyond Reptile: Meta-Learned Dot-Product Maximization between Gradients for Improved Single-Task Regularization
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Akhil Kedia
JobTitle#==#
Organization#==#Samsung Research
33, Seongchon-gil
Seocho-gu, Seoul
South Korea
Abstract#==#Meta-learning algorithms such as MAML, Reptile, and FOMAML have led to improved performance of several neural models. The primary difference between standard gradient descent and these meta-learning approaches is that they contain as a small component the gradient for maximizing dot-product between gradients of batches, leading to improved generalization. Previous work has shown that aligned gradients are related to generalization, and have also used the Reptile algorithm in a single-task setting to improve generalization. Inspired by these approaches for a single task setting, this paper proposes to use the finite differences first-order algorithm to calculate this gradient from dot-product of gradients, allowing explicit control on the weightage of this component relative to standard gradients. We use this gradient as a regularization technique, leading to more aligned gradients between different batches. By using the finite differences approximation, our approach does not suffer from O(n^2) memory usage of naively calculating the Hessian and can be easily applied to large models with large batch sizes. Our approach achieves state-of-the-art performance on the Gigaword dataset, and shows performance improvements on several datasets such as SQuAD-v2.0, Quasar-T, NewsQA and all the SuperGLUE datasets, with a range of models such as BERT, RoBERTa and ELECTRA. Our method also outperforms previous approaches of Reptile and FOMAML when used as a regularization technique, in both single and multi-task settings. Our method is model agnostic, and introduces no extra trainable weights.
Author{1}{Firstname}#=%=#Akhil
Author{1}{Lastname}#=%=#Kedia
Author{1}{Username}#=%=#akhilkedia94
Author{1}{Email}#=%=#akhil.kedia@samsung.com
Author{1}{Affiliation}#=%=#Samsung Electronics
Author{2}{Firstname}#=%=#Sai Chetan
Author{2}{Lastname}#=%=#Chinthakindi
Author{2}{Username}#=%=#saichetan
Author{2}{Email}#=%=#sai.chetan@samsung.com
Author{2}{Affiliation}#=%=#Samsung Electronics
Author{3}{Firstname}#=%=#Wonho
Author{3}{Lastname}#=%=#Ryu
Author{3}{Email}#=%=#wonho.ryu@samsung.com
Author{3}{Affiliation}#=%=#Samsung Research

==========