SubmissionNumber#=%=#3796
FinalPaperTitle#=%=#Human Rationales as Attribution Priors for Explainable Stance Detection
ShortPaperTitle#=%=#
NumberOfPages#=%=#15
CopyrightSigned#=%=#Sahil Jayaram
JobTitle#==#
Organization#==#Columbia University
New York, NY 10027
Abstract#==#As {NLP} systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier---particularly for inputs containing challenging phenomena such as sarcasm---at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model's predictions, thus serving as a computationally cheap and reliable source of attributions for our model.
Author{1}{Firstname}#=%=#Sahil
Author{1}{Lastname}#=%=#Jayaram
Author{1}{Username}#=%=#sahiljayaram
Author{1}{Email}#=%=#sahil.j@columbia.edu
Author{1}{Affiliation}#=%=#Columbia University
Author{2}{Firstname}#=%=#Emily
Author{2}{Lastname}#=%=#Allaway
Author{2}{Username}#=%=#eallaway
Author{2}{Email}#=%=#eallaway@cs.columbia.edu
Author{2}{Affiliation}#=%=#Columbia University

==========