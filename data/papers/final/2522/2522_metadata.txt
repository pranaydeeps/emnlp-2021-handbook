SubmissionNumber#=%=#2522
FinalPaperTitle#=%=#A {C}omprehensive {C}omparison of {W}ord {E}mbeddings in {E}vent & {E}ntity {C}oreference {R}esolution.
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#POUMAY
JobTitle#==#
Organization#==#ULIEGE / HEC Liege
Rue Louvrex 14
4000 Liege, Belgium
Abstract#==#Coreference Resolution is an important NLP task and most state-of-the-art methods rely on word embeddings for word representation. 
However, one issue that has been largely overlooked in literature is that of comparing  the performance of different embeddings across and within families. 
Therefore, we frame our study in the context of Event and Entity Coreference Resolution (EvCR \& EnCR), and address two questions   : 
1) Is there a trade-off between performance (predictive and run-time) and embedding size? 
2) How do the embeddings' performance compare within and across families?
Our experiments reveal several interesting findings. 
First, we observe diminishing returns in performance with respect to embedding size. 
E.g. a model using solely a character embedding achieves 86\% of the performance of the largest model (Elmo, GloVe, Character) while being 1.2\% of its size. 
Second, the larger models using multiple embeddings learns faster despite being slower per epoch. 
However, it is still slower at test time.
Finally, Elmo performs best on both EvCR and EnCR, while GloVe and FastText perform best in EvCR and EnCR respectively.
Author{1}{Firstname}#=%=#Judicael
Author{1}{Lastname}#=%=#POUMAY
Author{1}{Username}#=%=#jpoumay
Author{1}{Email}#=%=#judicael.poumay@uliege.be
Author{1}{Affiliation}#=%=#ULiège - HEC Liège
Author{2}{Firstname}#=%=#Ashwin
Author{2}{Lastname}#=%=#Ittoo
Author{2}{Username}#=%=#ashwinittoo
Author{2}{Email}#=%=#ashwin.ittoo@uliege.be
Author{2}{Affiliation}#=%=#ULiege

==========