SubmissionNumber#=%=#2990
FinalPaperTitle#=%=#BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#13
CopyrightSigned#=%=#Haoran Xu
JobTitle#==#
Organization#==#Johns Hopkins University
Abstract#==#The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of a dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT'14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT'14 dataset,  which exceeds all published numbers.
Author{1}{Firstname}#=%=#Haoran
Author{1}{Lastname}#=%=#Xu
Author{1}{Username}#=%=#coran123
Author{1}{Email}#=%=#hxu64@jhu.edu
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#Benjamin
Author{2}{Lastname}#=%=#Van Durme
Author{2}{Username}#=%=#vandurme
Author{2}{Email}#=%=#vandurme@cs.jhu.edu
Author{2}{Affiliation}#=%=#Johns Hopkins University
Author{3}{Firstname}#=%=#Kenton
Author{3}{Lastname}#=%=#Murray
Author{3}{Username}#=%=#kentonmurray
Author{3}{Email}#=%=#kenton@jhu.edu
Author{3}{Affiliation}#=%=#Johns Hopkins University

==========