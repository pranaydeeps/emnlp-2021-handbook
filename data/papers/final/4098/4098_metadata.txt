SubmissionNumber#=%=#4098
FinalPaperTitle#=%=#Numeracy enhances the Literacy of Language Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Avijit Thawani
JobTitle#==#
Organization#==#University of Southern California, Information Sciences Institute
Abstract#==#Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your `room' but not 500. Does a better grasp of numbers improve a model's understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy for evaluating literacy. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.
Author{1}{Firstname}#=%=#Avijit
Author{1}{Lastname}#=%=#Thawani
Author{1}{Username}#=%=#avijit_thawani
Author{1}{Email}#=%=#thawani@usc.edu
Author{1}{Affiliation}#=%=#University of Southern California
Author{2}{Firstname}#=%=#Jay
Author{2}{Lastname}#=%=#Pujara
Author{2}{Username}#=%=#puuj
Author{2}{Email}#=%=#jay@cs.umd.edu
Author{2}{Affiliation}#=%=#University of Southern California
Author{3}{Firstname}#=%=#Filip
Author{3}{Lastname}#=%=#Ilievski
Author{3}{Username}#=%=#filipilievski
Author{3}{Email}#=%=#ilievski@isi.edu
Author{3}{Affiliation}#=%=#USC Information Sciences Institute

==========