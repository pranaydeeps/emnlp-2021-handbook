SubmissionNumber#=%=#962
FinalPaperTitle#=%=#Exploiting Curriculum Learning in Unsupervised Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Jinliang Lu
JobTitle#==#
Organization#==#
Abstract#==#Back-translation (BT) has become one of the de facto components in unsupervised neural machine translation (UNMT), and it explicitly makes UNMT have translation ability. However, all the pseudo bi-texts generated by BT are treated equally as clean data during optimization without considering the quality diversity, leading to slow convergence and limited translation performance. To address this problem, we propose a curriculum learning method to gradually utilize pseudo bi-texts based on their quality from multiple granularities. Specifically, we first apply crosslingual word embedding to calculate the potential translation difficulty (quality) for the monolingual sentences. Then, the sentences are fed into UNMT from easy to hard batch by batch. Furthermore, considering the quality of sentences/tokens in a particular batch are also diverse, we further adopt the model itself to calculate the fine-grained quality scores, which are served as learning factors to balance the contributions of different parts when computing loss and encourage the UNMT model to focus on pseudo data with higher quality.  Experimental results on WMT 14 En-Fr, WMT 14 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks demonstrate that the proposed method achieves consistent improvements with faster convergence speed.
Author{1}{Firstname}#=%=#Jinliang
Author{1}{Lastname}#=%=#Lu
Author{1}{Username}#=%=#mr_lu2019
Author{1}{Email}#=%=#lujinliang2019@ia.ac.cn
Author{1}{Affiliation}#=%=#National Laboratory of Pattern Recognition, CASIA, Beijing, China
Author{2}{Firstname}#=%=#Jiajun
Author{2}{Lastname}#=%=#Zhang
Author{2}{Username}#=%=#jjzhang
Author{2}{Email}#=%=#jiajun.zhang@nlpr.ia.ac.cn
Author{2}{Affiliation}#=%=#Institute of Automation Chinese Academy of Sciences

==========