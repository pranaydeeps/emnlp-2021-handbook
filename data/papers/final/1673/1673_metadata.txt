SubmissionNumber#=%=#1673
FinalPaperTitle#=%=#Whatâ€™s in a Name? Answer Equivalence For Open-Domain Question Answering
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Chenglei Si
JobTitle#==#
Organization#==#
Abstract#==#A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers (i.e., equivalent answers). We incorporate answers for two settings: evaluation with additional answers and model training with equivalent answers. We analyse three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion increases the exact match score on all datasets for evaluation, while incorporating it helps model training over real-world datasets. We ensure the additional answers are valid through a human post hoc evaluation.
Author{1}{Firstname}#=%=#Chenglei
Author{1}{Lastname}#=%=#Si
Author{1}{Username}#=%=#si_chenglei
Author{1}{Email}#=%=#sichenglei1125@gmail.com
Author{1}{Affiliation}#=%=#University of Maryland, College Park
Author{2}{Firstname}#=%=#Chen
Author{2}{Lastname}#=%=#Zhao
Author{2}{Username}#=%=#zhaochen4321
Author{2}{Email}#=%=#henryzhao4321@gmail.com
Author{2}{Affiliation}#=%=#University of Maryland, college park
Author{3}{Firstname}#=%=#Jordan
Author{3}{Lastname}#=%=#Boyd-Graber
Author{3}{Username}#=%=#ezubaric
Author{3}{Email}#=%=#jbg@umiacs.umd.edu
Author{3}{Affiliation}#=%=#University of Maryland

==========