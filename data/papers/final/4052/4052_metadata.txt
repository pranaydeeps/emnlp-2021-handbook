SubmissionNumber#=%=#4052
FinalPaperTitle#=%=#Competing Independent Modules for Knowledge Integration and Optimization
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Parsa Bagherzaddeh
JobTitle#==#
Organization#==#Concordia University, Montreal, Canada
Abstract#==#This paper presents a neural framework of untied independent modules, used here for integrating off the shelf knowledge sources such as language models, lexica, POS information, and dependency relations. Each knowledge source is implemented as an independent component that can interact and share information with other knowledge sources. We report proof of concept experiments for several standard sentiment analysis tasks and show that the knowledge sources interoperate effectively without interference. As a second use-case, we show that the proposed framework is suitable for optimizing  BERT-like language models even without the help of external knowledge sources. We cast each Transformer layer as a separate module and demonstrate performance improvements from this explicit integration of the different information encoded at the different Transformer layers .
Author{1}{Firstname}#=%=#Parsa
Author{1}{Lastname}#=%=#Bagherzadeh
Author{1}{Username}#=%=#pbagherzadeh
Author{1}{Email}#=%=#p_bagher@encs.concordia.ca
Author{1}{Affiliation}#=%=#Concordia University
Author{2}{Firstname}#=%=#Sabine
Author{2}{Lastname}#=%=#Bergler
Author{2}{Username}#=%=#bergler
Author{2}{Email}#=%=#bergler@cse.concordia.ca
Author{2}{Affiliation}#=%=#Concordia University

==========