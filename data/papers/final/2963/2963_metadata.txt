SubmissionNumber#=%=#2963
FinalPaperTitle#=%=#Efficient Nearest Neighbor Language Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Junxian He
JobTitle#==#
Organization#==#
Abstract#==#Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.
Author{1}{Firstname}#=%=#Junxian
Author{1}{Lastname}#=%=#He
Author{1}{Username}#=%=#jxhe
Author{1}{Email}#=%=#junxianh@cs.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Graham
Author{2}{Lastname}#=%=#Neubig
Author{2}{Username}#=%=#gneubig
Author{2}{Email}#=%=#gneubig@cs.cmu.edu
Author{2}{Affiliation}#=%=#Carnegie Mellon University
Author{3}{Firstname}#=%=#Taylor
Author{3}{Lastname}#=%=#Berg-Kirkpatrick
Author{3}{Username}#=%=#tberg
Author{3}{Email}#=%=#tberg@eng.ucsd.edu
Author{3}{Affiliation}#=%=#University of California San Diego

==========