SubmissionNumber#=%=#3306
FinalPaperTitle#=%=#How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#David Anthony Bau
JobTitle#==#
Organization#==#
Abstract#==#After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a lexical context model in which generalization is consistent with the last word observed, and a syntactic context model in which generalization is consistent with the global structure of the input. In experiments in English, Finnish, Mandarin, and random regular languages, we demonstrate that neural language models interpolate between these two forms of generalization: their predictions are well-approximated by a log-linear combination of lexical and syntactic predictive distributions. We then show that, in some languages, noise mediates the two forms of generalization: noise applied to input tokens encourages syntactic generalization, while noise in history representations encourages lexical generalization. Finally, we offer a preliminary theoretical explanation of these results by proving that the observed interpolation behavior is expected in log-linear models with a particular feature correlation structure. These results help explain the effectiveness of two popular regularization schemes and show that aspects of sequence model generalization can be understood and controlled.
Author{1}{Firstname}#=%=#D. Anthony
Author{1}{Lastname}#=%=#Bau
Author{1}{Username}#=%=#abau
Author{1}{Email}#=%=#abau@mit.edu
Author{1}{Affiliation}#=%=#Massachusetts Institute of Technology
Author{2}{Firstname}#=%=#Jacob
Author{2}{Lastname}#=%=#Andreas
Author{2}{Username}#=%=#jacobandreas
Author{2}{Email}#=%=#jda@mit.edu
Author{2}{Affiliation}#=%=#MIT

==========