SubmissionNumber#=%=#4537
FinalPaperTitle#=%=#A Thorough Evaluation of Task-Specific Pretraining for Summarization
ShortPaperTitle#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Sascha Rothe
JobTitle#==#
Organization#==#
Abstract#==#Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al.,2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et al.,2020). We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study. We also extend our study to a low resource and zero shot setup, to understand how many training examples are needed in order to ablate the task-specific pretraining without quality loss. Our results show that task-agnostic pretraining is sufficient for most cases which hopefully reduces the need for costly task-specific pretraining. We also report new state-of-the-art number for two summarization task using a T5 model with 11 billion parameters and an optimal beam search length penalty.
Author{1}{Firstname}#=%=#Sascha
Author{1}{Lastname}#=%=#Rothe
Author{1}{Username}#=%=#sascha
Author{1}{Email}#=%=#rothe@google.com
Author{1}{Affiliation}#=%=#Google
Author{2}{Firstname}#=%=#Joshua
Author{2}{Lastname}#=%=#Maynez
Author{2}{Username}#=%=#joshuahm
Author{2}{Email}#=%=#joshuahm@google.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Shashi
Author{3}{Lastname}#=%=#Narayan
Author{3}{Username}#=%=#shashinarayan
Author{3}{Email}#=%=#shashinarayan@google.com
Author{3}{Affiliation}#=%=#Google

==========