SubmissionNumber#=%=#909
FinalPaperTitle#=%=#Dynamic Knowledge Distillation for Pre-trained Language Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Lei Li
JobTitle#==#
Organization#==#
Abstract#==#Knowledge distillation~(KD) has been proved effective for compressing large-scale pre-trained language models.
However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset.
In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency.
We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation.
Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10\% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective.
We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.
Author{1}{Firstname}#=%=#Lei
Author{1}{Lastname}#=%=#Li
Author{1}{Username}#=%=#tobiaslee
Author{1}{Email}#=%=#lilei@stu.pku.edu.cn
Author{1}{Affiliation}#=%=#Peking University
Author{2}{Firstname}#=%=#Yankai
Author{2}{Lastname}#=%=#Lin
Author{2}{Username}#=%=#lyk423
Author{2}{Email}#=%=#yankailin@tencent.com
Author{2}{Affiliation}#=%=#Pattern Recognition Center, WeChat, Tencent
Author{3}{Firstname}#=%=#Shuhuai
Author{3}{Lastname}#=%=#Ren
Author{3}{Username}#=%=#shuhuai-ren
Author{3}{Email}#=%=#renshuhuai007@gmail.com
Author{3}{Affiliation}#=%=#Peking University
Author{4}{Firstname}#=%=#Peng
Author{4}{Lastname}#=%=#Li
Author{4}{Username}#=%=#lipeng17
Author{4}{Email}#=%=#patrickpli@tencent.com
Author{4}{Affiliation}#=%=#WeChat AI, Tencent Inc., China
Author{5}{Firstname}#=%=#Jie
Author{5}{Lastname}#=%=#Zhou
Author{5}{Username}#=%=#jerryitp
Author{5}{Email}#=%=#withtomzhou@tencent.com
Author{5}{Affiliation}#=%=#Tencent Inc.
Author{6}{Firstname}#=%=#Xu
Author{6}{Lastname}#=%=#Sun
Author{6}{Username}#=%=#xusun26
Author{6}{Email}#=%=#xusun@pku.edu.cn
Author{6}{Affiliation}#=%=#Peking University

==========