SubmissionNumber#=%=#2703
FinalPaperTitle#=%=#Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Tomasz Limsiewicz
JobTitle#==#
Organization#==#Institute of Formal and Applied Linguistics
Faculty of Mathematics and PhysicsCharles University
Malostranské náměstí 25
Prague, Czech Republic
Abstract#==#State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages.
The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages.
We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.
Author{1}{Firstname}#=%=#Tomasz
Author{1}{Lastname}#=%=#Limisiewicz
Author{1}{Username}#=%=#tomlim
Author{1}{Email}#=%=#limisiewicz@ufal.mff.cuni.cz
Author{1}{Affiliation}#=%=#Charles University in Prague
Author{2}{Firstname}#=%=#David
Author{2}{Lastname}#=%=#Mareček
Author{2}{Username}#=%=#marecek
Author{2}{Email}#=%=#marecek@ufal.mff.cuni.cz
Author{2}{Affiliation}#=%=#Charles University

==========