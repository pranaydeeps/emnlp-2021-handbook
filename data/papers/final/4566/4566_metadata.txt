SubmissionNumber#=%=#4566
FinalPaperTitle#=%=#CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning
ShortPaperTitle#=%=#
NumberOfPages#=%=#11
CopyrightSigned#=%=#Zhenxi Lin
JobTitle#==#
Organization#==#
Abstract#==#Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CATE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained model to obtain a contextual representation of target words and employs a contrastive objective to promote an increased distance between target words' literal and metaphorical senses based on linguistic theories. Furthermore, we propose a simple strategy to collect large-scale candidate instances from the general corpus and generalize the model via self-training. Extensive experiments show that CATE achieves better performance against state-of-the-art baselines on several benchmark datasets.
Author{1}{Firstname}#=%=#Zhenxi
Author{1}{Lastname}#=%=#Lin
Author{1}{Username}#=%=#linzhenxi
Author{1}{Email}#=%=#786450794@qq.com
Author{1}{Affiliation}#=%=#South China University of Technology
Author{2}{Firstname}#=%=#Qianli
Author{2}{Lastname}#=%=#Ma
Author{2}{Username}#=%=#mql206
Author{2}{Email}#=%=#qianlima@scut.edu.cn
Author{2}{Affiliation}#=%=#South China University of Technology
Author{3}{Firstname}#=%=#Jiangyue
Author{3}{Lastname}#=%=#Yan
Author{3}{Username}#=%=#yalunar
Author{3}{Email}#=%=#584073853@qq.com
Author{3}{Affiliation}#=%=#South China University of Technology
Author{4}{Firstname}#=%=#JIEYU
Author{4}{Lastname}#=%=#CHEN
Author{4}{Username}#=%=#jieyuchen
Author{4}{Email}#=%=#18043507r@connect.polyu.hk
Author{4}{Affiliation}#=%=#Hong Kong Polytechnic University

==========