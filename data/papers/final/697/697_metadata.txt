SubmissionNumber#=%=#697
FinalPaperTitle#=%=#Pragmatic Neural Language Modelling in Machine Translation
ShortPaperTitle#=%=#Pragmatic Neural Language Modelling in Machine Translation
NumberOfPages#=%=#10
CopyrightSigned#=%=#Paul Baltescu
JobTitle#==#
Organization#==#University of Oxford
Abstract#==#This paper presents an in-depth investigation on integrating neural language
models in translation systems. Scaling neural language models is a difficult
task, but crucial for real-world applications. This paper evaluates the impact
on end-to-end MT quality of both new and existing scaling techniques. We show
when explicitly normalising neural models is necessary and what optimisation
tricks one should use in such scenarios. We also focus on scalable training
algorithms and investigate noise contrastive estimation and diagonal contexts
as sources for further speed improvements. We explore the trade-offs between
neural models and back-off n-gram models and find that neural models make
strong candidates for natural language applications in memory constrained
environments, yet still lag behind traditional models in raw translation
quality. We conclude with a set of recommendations one should follow to build a
scalable neural language model for MT.
Author{1}{Firstname}#=%=#Paul
Author{1}{Lastname}#=%=#Baltescu
Author{1}{Email}#=%=#pauldb89@gmail.com
Author{1}{Affiliation}#=%=#University of Oxford
Author{2}{Firstname}#=%=#Phil
Author{2}{Lastname}#=%=#Blunsom
Author{2}{Email}#=%=#phil.blunsom@cs.ox.ac.uk
Author{2}{Affiliation}#=%=#University of Oxford

==========