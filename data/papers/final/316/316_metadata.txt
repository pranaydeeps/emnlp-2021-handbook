SubmissionNumber#=%=#316
FinalPaperTitle#=%=#Finetuning Pretrained Transformers into RNNs
ShortPaperTitle#=%=#
NumberOfPages#=%=#14
CopyrightSigned#=%=#Jungo Kasai
JobTitle#==#
Organization#==#Paul G. Allen School of Computer Science & Engineering, University of Washington
185 E Stevens Way NE, Seattle, WA 98195, USA
Abstract#==#Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanismâ€™s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.
Author{1}{Firstname}#=%=#Jungo
Author{1}{Lastname}#=%=#Kasai
Author{1}{Username}#=%=#jungokasai
Author{1}{Email}#=%=#jkasai@cs.washington.edu
Author{1}{Affiliation}#=%=#University of Washington
Author{2}{Firstname}#=%=#Hao
Author{2}{Lastname}#=%=#Peng
Author{2}{Username}#=%=#haopeng
Author{2}{Email}#=%=#hapeng@cs.washington.edu
Author{2}{Affiliation}#=%=#University of Washington
Author{3}{Firstname}#=%=#Yizhe
Author{3}{Lastname}#=%=#Zhang
Author{3}{Username}#=%=#yizhe
Author{3}{Email}#=%=#jeremy071242044@gmail.com
Author{3}{Affiliation}#=%=#Microsoft
Author{4}{Firstname}#=%=#Dani
Author{4}{Lastname}#=%=#Yogatama
Author{4}{Username}#=%=#dyogatama
Author{4}{Email}#=%=#dyogatama@google.com
Author{4}{Affiliation}#=%=#DeepMind
Author{5}{Firstname}#=%=#Gabriel
Author{5}{Lastname}#=%=#Ilharco
Author{5}{Username}#=%=#gamaga
Author{5}{Email}#=%=#gamaga@cs.washington.edu
Author{5}{Affiliation}#=%=#University of Washington
Author{6}{Firstname}#=%=#Nikolaos
Author{6}{Lastname}#=%=#Pappas
Author{6}{Username}#=%=#nik0spapp
Author{6}{Email}#=%=#npappas@cs.washington.edu
Author{6}{Affiliation}#=%=#University of Washington
Author{7}{Firstname}#=%=#Yi
Author{7}{Lastname}#=%=#Mao
Author{7}{Username}#=%=#yimao
Author{7}{Email}#=%=#frente@gmail.com
Author{7}{Affiliation}#=%=#Microsoft
Author{8}{Firstname}#=%=#Weizhu
Author{8}{Lastname}#=%=#Chen
Author{8}{Username}#=%=#chenweizhu
Author{8}{Email}#=%=#wzchen@microsoft.com
Author{8}{Affiliation}#=%=#Microsoft
Author{9}{Firstname}#=%=#Noah A.
Author{9}{Lastname}#=%=#Smith
Author{9}{Username}#=%=#nasmith
Author{9}{Email}#=%=#nasmith@cs.washington.edu
Author{9}{Affiliation}#=%=#University of Washington

==========