SubmissionNumber#=%=#1813
FinalPaperTitle#=%=#Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding
ShortPaperTitle#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Zi-Yi Dou
JobTitle#==#
Organization#==#
Abstract#==#Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing representation generality.
Author{1}{Firstname}#=%=#Zi-Yi
Author{1}{Lastname}#=%=#Dou
Author{1}{Username}#=%=#douglas0830
Author{1}{Email}#=%=#zdou0830@gmail.com
Author{1}{Affiliation}#=%=#UCLA
Author{2}{Firstname}#=%=#Nanyun
Author{2}{Lastname}#=%=#Peng
Author{2}{Username}#=%=#npeng
Author{2}{Email}#=%=#violetpeng@cs.ucla.edu
Author{2}{Affiliation}#=%=#University of California, Los Angeles

==========