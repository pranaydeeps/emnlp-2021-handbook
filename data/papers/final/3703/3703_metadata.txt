SubmissionNumber#=%=#3703
FinalPaperTitle#=%=#Multi-Vector Attention Models for Deep Re-ranking
ShortPaperTitle#=%=#
NumberOfPages#=%=#5
CopyrightSigned#=%=#Giulio Zhou
JobTitle#==#
Organization#==#
Abstract#==#Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is based on the joint query-document representation. Dual encoder models are typically used for retrieval and deep re-ranking, while cross-attention models are typically used for shallow re-ranking. In this paper, we present a lightweight architecture that explores this joint cost vs. accuracy trade-off based on multi-vector attention (MVA). We thoroughly evaluate our method on the MS-MARCO passage retrieval dataset and show how to efficiently trade off retrieval accuracy with joint computation and offline document storage cost. We show that a highly compressed document representation and inexpensive joint computation can be achieved through a combination of learned pooling tokens and aggressive downprojection. Our code and model checkpoints  are open-source and available on GitHub.
Author{1}{Firstname}#=%=#Giulio
Author{1}{Lastname}#=%=#Zhou
Author{1}{Username}#=%=#gzhou
Author{1}{Email}#=%=#gzz@andrew.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Jacob
Author{2}{Lastname}#=%=#Devlin
Author{2}{Username}#=%=#jdevlin
Author{2}{Email}#=%=#jacobdevlin@google.com
Author{2}{Affiliation}#=%=#Google

==========