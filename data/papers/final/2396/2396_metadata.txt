SubmissionNumber#=%=#2396
FinalPaperTitle#=%=#Revisiting Tri-training of Dependency Parsers
ShortPaperTitle#=%=#
NumberOfPages#=%=#17
CopyrightSigned#=%=#Joachim Wagner
JobTitle#==#
Organization#==#Dublin City University, Glasnevin, Dublin, County Dublin, Ireland
Abstract#==#We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised learning can be expected to have the most impact here. Based on treebank size and available ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and Vietnamese. Furthermore, we include English in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.
Author{1}{Firstname}#=%=#Joachim
Author{1}{Lastname}#=%=#Wagner
Author{1}{Username}#=%=#jwagner
Author{1}{Email}#=%=#jwagner@computing.dcu.ie
Author{1}{Affiliation}#=%=#ADAPT Centre, Dublin City University
Author{2}{Firstname}#=%=#Jennifer
Author{2}{Lastname}#=%=#Foster
Author{2}{Username}#=%=#jfoster
Author{2}{Email}#=%=#fosterjen@gmail.com
Author{2}{Affiliation}#=%=#Dublin City University

==========