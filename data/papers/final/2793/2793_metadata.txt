SubmissionNumber#=%=#2793
FinalPaperTitle#=%=#How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Sara Rajaee
JobTitle#==#
Organization#==#
Abstract#==#It is widely accepted that fine-tuning pre-trained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective.
Author{1}{Firstname}#=%=#Sara
Author{1}{Lastname}#=%=#Rajaee
Author{1}{Username}#=%=#sara_rajaee
Author{1}{Email}#=%=#sara_rajaee@comp.iust.ac.ir
Author{1}{Affiliation}#=%=#Iran University of Science and Technology
Author{2}{Firstname}#=%=#Mohammad Taher
Author{2}{Lastname}#=%=#Pilehvar
Author{2}{Username}#=%=#pilehvar
Author{2}{Email}#=%=#mp792@cam.ac.uk
Author{2}{Affiliation}#=%=#Tehran Institute for Advanced Studies

==========