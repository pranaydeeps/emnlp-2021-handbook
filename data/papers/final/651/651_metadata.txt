SubmissionNumber#=%=#651
FinalPaperTitle#=%=#DILBERT: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Roi Reichart
JobTitle#==#
Organization#==#
Abstract#==#The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model on a source domain  and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP tasks, the output categories substantially differ between domains, making adaptation even more challenging. This, for example, happens in the task of aspect extraction, where the aspects of interest of reviews of, e.g., restaurants or electronic devices may be very different.  

This paper presents a new fine-tuning scheme for BERT, which aims to address the above challenges. We name this scheme  DILBERT: Domain Invariant Learning with BERT, and customize it  for aspect extraction in the unsupervised domain adaptation setting. DILBERT harnesses the categorical information of both the source and the target domains to guide the pre-training process towards a more domain and category invariant representation, thus closing the gap between the domains.  We show that DILBERT yields substantial improvements over state-of-the-art baselines while using a fraction of the unlabeled data, particularly in more challenging domain adaptation setups.
Author{1}{Firstname}#=%=#Entony
Author{1}{Lastname}#=%=#Lekhtman
Author{1}{Username}#=%=#tonylek
Author{1}{Email}#=%=#tony1@campus.technion.ac.il
Author{1}{Affiliation}#=%=#Technion - Israel Institute of Technology
Author{2}{Firstname}#=%=#Yftah
Author{2}{Lastname}#=%=#Ziser
Author{2}{Username}#=%=#yftah89
Author{2}{Email}#=%=#yftah89@gmail.com
Author{2}{Affiliation}#=%=#Facebook
Author{3}{Firstname}#=%=#Roi
Author{3}{Lastname}#=%=#Reichart
Author{3}{Username}#=%=#roiri
Author{3}{Email}#=%=#roireichart@gmail.com
Author{3}{Affiliation}#=%=#Technion - Israel Institute of Technology

==========