SubmissionNumber#=%=#2383
FinalPaperTitle#=%=#Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Wangchunshu
JobTitle#==#
Organization#==#
Abstract#==#In this paper, we propose \textbf{S}equence \textbf{S}pan \textbf{R}ewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.
Author{1}{Firstname}#=%=#Wangchunshu
Author{1}{Lastname}#=%=#Zhou
Author{1}{Username}#=%=#michaelzw
Author{1}{Email}#=%=#wcszhou@stanford.edu
Author{1}{Affiliation}#=%=#Beihang Univeristy
Author{2}{Firstname}#=%=#Tao
Author{2}{Lastname}#=%=#Ge
Author{2}{Username}#=%=#getao
Author{2}{Email}#=%=#tage@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft
Author{3}{Firstname}#=%=#Canwen
Author{3}{Lastname}#=%=#Xu
Author{3}{Username}#=%=#jetrunner
Author{3}{Email}#=%=#cxu@ucsd.edu
Author{3}{Affiliation}#=%=#UC San Diego
Author{4}{Firstname}#=%=#Ke
Author{4}{Lastname}#=%=#Xu
Author{4}{Username}#=%=#kexubuaa
Author{4}{Email}#=%=#kexu@nlsde.buaa.edu.cn
Author{4}{Affiliation}#=%=#Beihang University
Author{5}{Firstname}#=%=#Furu
Author{5}{Lastname}#=%=#Wei
Author{5}{Username}#=%=#fuwei
Author{5}{Email}#=%=#fuwei@microsoft.com
Author{5}{Affiliation}#=%=#Microsoft Research

==========