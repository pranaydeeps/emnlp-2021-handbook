SubmissionNumber#=%=#3001
FinalPaperTitle#=%=#Sequence Length is a Domain: Length-based Overfitting in Transformer Models
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Dušan Variš
JobTitle#==#
Organization#==#Faculty of Mathematics and Physics, Charles University, Malostranské náměstí 25, 118 00 Prague, Czechia
Abstract#==#Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training.
In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data.
Additionally, Transformer and other architectures are known to struggle when generating very long sequences.
For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches  (Koehn and Knowles, 2017).

We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data.
We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data.
Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.
Author{1}{Firstname}#=%=#Dusan
Author{1}{Lastname}#=%=#Varis
Author{1}{Username}#=%=#varis
Author{1}{Email}#=%=#varis@ufal.mff.cuni.cz
Author{1}{Affiliation}#=%=#Charles University, Institute of Formal and Applied Linguistics
Author{2}{Firstname}#=%=#Ondřej
Author{2}{Lastname}#=%=#Bojar
Author{2}{Username}#=%=#bojar
Author{2}{Email}#=%=#bojar@ufal.mff.cuni.cz
Author{2}{Affiliation}#=%=#Charles University, MFF UFAL

==========