SubmissionNumber#=%=#703
FinalPaperTitle#=%=#When differential privacy meets NLP: The devil is in the detail
ShortPaperTitle#=%=#
NumberOfPages#=%=#7
CopyrightSigned#=%=#Ivan Habernal
JobTitle#==#
Organization#==#Technical University of Darmstadt, Germany
Abstract#==#Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users' original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder's dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.
Author{1}{Firstname}#=%=#Ivan
Author{1}{Lastname}#=%=#Habernal
Author{1}{Username}#=%=#ivan.habernal
Author{1}{Email}#=%=#habernal@ukp.informatik.tu-darmstadt.de
Author{1}{Affiliation}#=%=#Technische Universit√§t Darmstadt

==========