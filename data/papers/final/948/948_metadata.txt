SubmissionNumber#=%=#948
FinalPaperTitle#=%=#Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy
ShortPaperTitle#=%=#
NumberOfPages#=%=#12
CopyrightSigned#=%=#Shaolei Zhang
JobTitle#==#
Organization#==#Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)
University of Chinese Academy of Sciences, Beijing, China
Abstract#==#Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.
Author{1}{Firstname}#=%=#Shaolei
Author{1}{Lastname}#=%=#Zhang
Author{1}{Username}#=%=#zhangshaolei
Author{1}{Email}#=%=#zhangshaolei20z@ict.ac.cn
Author{1}{Affiliation}#=%=#Institute of Computing Technology, Chinese Academy of Sciences
Author{2}{Firstname}#=%=#Yang
Author{2}{Lastname}#=%=#Feng
Author{2}{Username}#=%=#y.feng
Author{2}{Email}#=%=#fengyang@ict.ac.cn
Author{2}{Affiliation}#=%=#Institute of Computing Technology, Chinese Academy of Sciences

==========