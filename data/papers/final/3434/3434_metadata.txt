SubmissionNumber#=%=#3434
FinalPaperTitle#=%=#Sequence-to-Lattice Models for Fast Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#8
CopyrightSigned#=%=#Yuntian Deng
JobTitle#==#
Organization#==#
Abstract#==#Non-autoregressive machine translation (NAT) approaches enable fast generation by utilizing parallelizable generative processes. The remaining bottleneck in these models is their decoder layers; unfortunately unlike in autoregressive models (Kasai et al.,  2020), removing decoder layers from NAT models significantly degrades accuracy. This work proposes a sequence-to-lattice model that replaces the decoder with a search lattice.  Our approach first constructs a candidate lattice using efficient lookup operations, generates lattice scores from a deep encoder, and finally finds the best path using dynamic programming. Experiments on three machine translation datasets show that our method is faster than past non-autoregressive generation approaches, and more accurate than naively reducing the number of decoder layers.
Author{1}{Firstname}#=%=#Yuntian
Author{1}{Lastname}#=%=#Deng
Author{1}{Username}#=%=#cloudyclouds
Author{1}{Email}#=%=#dengyuntian@seas.harvard.edu
Author{1}{Affiliation}#=%=#Harvard University
Author{2}{Firstname}#=%=#Alexander
Author{2}{Lastname}#=%=#Rush
Author{2}{Username}#=%=#srush
Author{2}{Email}#=%=#arush@cornell.edu
Author{2}{Affiliation}#=%=#Cornell University

==========