SubmissionNumber#=%=#1961
FinalPaperTitle#=%=#SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks
ShortPaperTitle#=%=#
NumberOfPages#=%=#20
CopyrightSigned#=%=#Wanyu Du
JobTitle#==#
Organization#==#
Abstract#==#Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods lead to high computation cost and can easily get overfitted on small training sets, while weighted-decoding methods are inherently constrained by the low-variance high-bias pre-trained model. In this work, we propose a novel approach to control the generation of Transformer-based pre-trained language models: the SideControl framework, which leverages a novel control attributes loss to incorporate useful control signals, and is shown to perform well with very limited training samples. We evaluate our proposed method on two benchmark open-domain dialogue datasets, and results show that the SideControl framework has better controllability, higher generation quality and better sample-efficiency than existing gradient-based and weighted-decoding baselines.
Author{1}{Firstname}#=%=#Wanyu
Author{1}{Lastname}#=%=#Du
Author{1}{Username}#=%=#duwanyu0706
Author{1}{Email}#=%=#wd5jq@virginia.edu
Author{1}{Affiliation}#=%=#University of Virginia
Author{2}{Firstname}#=%=#Yangfeng
Author{2}{Lastname}#=%=#Ji
Author{2}{Username}#=%=#yangfengji
Author{2}{Email}#=%=#yangfeng@virginia.edu
Author{2}{Affiliation}#=%=#University of Virginia

==========