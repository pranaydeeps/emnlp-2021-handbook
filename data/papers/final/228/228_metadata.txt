SubmissionNumber#=%=#228
FinalPaperTitle#=%=#Simple task-specific bilingual word embeddings
ShortPaperTitle#=%=#Simple task-specific bilingual word embeddings
NumberOfPages#=%=#5
CopyrightSigned#=%=#Anders Søgaard
JobTitle#==#
Organization#==#University of Copenhagen
Njalsgade 140
DK-2300 Copenhagen
Abstract#==#We introduce a simple wrapper method that uses off-the-shelf word embedding
algorithms to learn task-specific bilingual word embeddings. We use a small
dictionary to produce mixed context-target pairs that we use to train embedding
models. The model has the advantage that it (a) is independent of the choice of
embedding algorithm, (b) does not require parallel data, and (c) can be adapted
to specific tasks by re-defining the equivalence classes. We show how our
method outperforms off-the-shelf bilingual embeddings on the task of
unsupervised cross-language part-of-speech (POS) tagging, as well as on the
task of semi-supervised cross-language super sense (SuS) tagging.
Author{1}{Firstname}#=%=#Stephan
Author{1}{Lastname}#=%=#Gouws
Author{1}{Email}#=%=#gouwsmeister@gmail.com
Author{1}{Affiliation}#=%=#Stellenbosch University
Author{2}{Firstname}#=%=#Anders
Author{2}{Lastname}#=%=#Søgaard
Author{2}{Email}#=%=#soegaard@hum.ku.dk
Author{2}{Affiliation}#=%=#University of Copenhagen

==========