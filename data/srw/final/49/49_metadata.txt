SubmissionNumber#=%=#49
FinalPaperTitle#=%=#Discourse and Document-level Information for Evaluating Language Output Tasks
ShortPaperTitle#=%=#Discourse and Document-level Information for Evaluating Language Output Tasks
NumberOfPages#=%=#8
CopyrightSigned#=%=#Carolina Scarton
JobTitle#==#Discourse and Document-level Information for Evaluating Language Output Tasks
Organization#==#Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello, Sheffield, S1 4DP, UK
Abstract#==#Evaluating the quality of language output tasks such as Machine Translation
(MT)
and Automatic Summarisation (AS) is a challenging topic in Natural Language
Processing (NLP). Recently, techniques focusing only on the use of outputs of
the systems and source information have been investigated. In MT, this is
referred to as Quality Estimation (QE), an approach based on using machine
learning techniques to predict the quality of unseen data, generalising from a
few labelled data points. Traditional QE research addresses sentence-level QE
evaluation and prediction, disregarding document-level information.
Document-level QE requires a different set up from sentence-level, which makes
the study of appropriate quality scores, features and models necessary. Our aim
is to explore document-level QE of MT, focusing on discourse information.
However, the findings of this research can improve other NLP tasks, such as AS.
Author{1}{Firstname}#=%=#Carolina
Author{1}{Lastname}#=%=#Scarton
Author{1}{Email}#=%=#carol.scarton@gmail.com
Author{1}{Affiliation}#=%=#University of Sheffield

==========