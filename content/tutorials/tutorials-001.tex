\begin{bio}
  {\bfseries Alane Suhr} is a PhD student at Cornell University who's research focuses on grounded natural language understanding. Alane has designed crowdsourcing tasks for collecting language data to study situated natural language understanding. Alane co-presented a tutorial in ACL 2018.

  {\bfseries Clara Vania} is an applied scientist at Amazon. Her research focuses on crowdsourcing, transfer learning, and multilingual NLU. Recently, she has been working on semi-automatic data collection for natural language inference and crowdsourcing methods for question answering.

  {\bfseries Nikita Nangia} is a PhD student at New York University. NikitaÕs work focuses on crowdsourcing methods and data creation for natural language understanding. Her recent work explores using incentive structures to illicit creative examples. Nikita co-organized a tutorial on latent structure models for NLP at ACL 2019.

    {\bfseries Maarten Sap} is a PhD student at the University of Washington.
His research focuses on endowing NLP systems with social intelligence and social commonsense, and understanding social inequality and bias in language. His substantial experience with crowdsourcing includes the collecting of the SOCIALIQA commonsense benchmark as well as the creation of knowledge graphs with inferential knowledge (ATOMIC, Social Bias Frames).

  {\bfseries Mark Yatskar} is an assistant professor at the University of Pennsylvania. His research focuses on the intersection of natural language processing and computer vision. MarkÕs work has resulted in the creation of datasets such as imSitu, QuAC and WinoBias and recent research has focused on gender bias in visual recognition and coreference resolution.

  {\bfseries Sam Bowman} is an assistant professor at New York University.
Sam works on data creation, benchmarking, and model analysis for NLU and computational linguistics. Sam has had a substantial role in several NLU datasets, including SNLI, MNLI, XNLI, CoLA, and BLiMP, and his recent work has focused on experimentally evaluating methods for crowdsourced corpus construction.

  {\bfseries Yoav Artzi} is an associate professor at Cornell University.
YoavÕs research focuses on learning expressive models for natural language understanding, most recently in situated interactive scenarios. Yoav led tutorials on semantic parsing in ACL 2013, EMNLP 2014 and AAAI 2015.

\end{bio}

\begin{tutorial}
  {Crowdsourcing Beyond Annotation: Case Studies in Benchmark Data Collection}
  {tutorial-final-01}
  {\daydateyear, \tutorialmorningtime}
  {\TutLocA}

Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering, instruction following, visual reasoning, and commonsense reasoning. Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. The goal of this tutorial is to expose NLP researchers to such data collection crowdsourcing methods and principles through a detailed discussion of a diverse set of case studies.

\end{tutorial}