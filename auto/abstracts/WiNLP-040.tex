With synthetic data generation, the required amount of human-generated training data can be reduced significantly. In this work, we explore the usage of automatic paraphrasing models such as GPT-2 and CVAE to directly augment the template carrier phrases for task-oriented dialogue systems while preserving the slots, and we systematically analyze how far manually annotated training data can be reduced. We extrinsically evaluate the performance of a natural language understanding systems on augmented data on various levels of data availability, reducing manually written templates up to 75 percent while preserving the accuracy. We further point out that the typical NLG quality metrics such as BLEU, utterance similarity, or utterance perplexity, are not suitable to assess the intrinsic quality of NLU paraphrases, and that the public task-oriented NLU datasets such as ATIS and SNIPS have severe limitations.
