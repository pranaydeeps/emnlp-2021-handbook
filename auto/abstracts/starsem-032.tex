The lexical semantic relationships between word pairs are key features for many NLP tasks.  Most approaches for automatically classifying related word pairs are hindered by data sparsity because of their need to observe two words co-occurring for detecting the lexical relation holding between them. Even when mining very large corpora, not every related word pair co-occurs.  Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even those never found in the same sentence in a given corpus.  In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving significant gains in recall.
