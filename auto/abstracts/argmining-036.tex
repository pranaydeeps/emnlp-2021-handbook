Argument role labeling is a fundamental task001in Argument Mining research.  However, such002research  often  suffers  from  a  lack  of  large-003scale datasets labeled for argument roles such004as evidence, which is crucial for neural model005training.While  large  pretrained  language006models  have  somewhat  alleviated  the  need007for  massive  manually  labeled  datasets,  how008much  these  models  can  further  benefit  from009self-training techniques hasn't been widely ex-010plored  in  the  literature  in  general  and  in  Ar-011gument Mining specifically.  In this work, we012focus  on  self-trained  language  models  (par-013ticularly  BERT)  for  evidence  detection.   We014provide  a  thorough  investigation  on  how  to015utilize  pseudo  labels  effectively  in  the  self-016training  scheme.We  also  assess  whether017adding  pseudo  labels  from  an  out-of-domain018source can be beneficial.  Experiments on sen-019tence level evidence detection show that self-020training can complement pretrained language021models to provide performance improvements.
