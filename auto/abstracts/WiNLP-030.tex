Current machine translation methods struggle to translate phrases that contain words that appear infrequently in the training data. Low-resource languages in particular are harmed by this because they have such little data that rare words may only be seen once. This issue can be partly addressed using approaches that explicitly incorporate word-level translation lexicons (Akyurek and Andreas, 2021). However, existing approaches for constructing these lexicons require either substantial human annotation or large datasets. We propose a model for few-shot learning of lexical translation rules. Our approach enables neural machine translation models to efficiently learn translations of infrequent words. In experiments on low-resource Englishâ†’Spanish translation, our approach yields BLEU improvements on sentences containing words seen only once during training.
