In this paper, we conduct experiments with Proximal Policy Optimization (PPO), a reinforcement learning algorithm, for detoxifying language models. We fine-tune pre-trained language models with PPO using a reward model trained from human assessment of toxicity. We measure and observe a dip in toxicity and stereotypical bias in the detoxified language models.
