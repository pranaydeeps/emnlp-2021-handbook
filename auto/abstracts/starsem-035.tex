The Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an over-counting of the predicate's contribution to the meaning of the phrase. We investigate two possible solutions and find that one solution (exclusion of simple lexical vector at test time) significantly improves performance on two out of the three composition datasets.
