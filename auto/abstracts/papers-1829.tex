Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called ''Turing Test'' problem for neural text generation methods. In this work, we present the TURINGBENCH benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2\_small, GPT-2\_medium, GPT-2\_large,GPT-2\_xl, GPT-2\_PyTorch, GPT-3, GROVER\_base, GROVER\_large, GROVER\_mega, CTRL, XLM, XLNET\_base, XLNET\_large, FAIR\_wmt19, FAIR\_wmt20, TRANSFORMER\_XL, PPLM\_distil, PPLM\_gpt2}, (2) two benchmark tasks--i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using  TURINGBENCH  show that GPT-3 and FAIR\_wmt20 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TURINGBENCH  is available at: https://turingbench.ist.psu.edu/
