There are numerous applications for the prediction of new links within knowledge graphs. The best models for this task, knowledge graph embedding (KGE) models, are typically evaluated by computing some average performance metric on a test set. This approach makes it difficult to identify where the models might systematically fail. To combat this issue, we propose a new evaluation framework that builds upon the idea of behavioral testing, which allows us to target critical system capabilities and evaluate the performance of KGE models on them.
