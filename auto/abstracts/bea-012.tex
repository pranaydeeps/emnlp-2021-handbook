Various measures have been used to evaluate the effectiveness of automated text scoring (ATS) systems with respect to a human gold standard. However, there is no systematic study comparing the efficacy of these metrics under different experimental conditions. In this paper we first argue that measures of agreement are more appropriate than measures of association (i.e., correlation) for measuring the effectiveness of ATS systems. We then present a thorough review and analysis of frequently used measures of agreement. We outline desirable properties for measuring the effectiveness of an ATS system, and experimentally demonstrate using both synthetic and real ATS data, that some commonly used mea- sures (e.g., Cohen's kappa) lack these properties. Finally, we identify the most appropriate measures of agreement and present general recommendations for best evaluation practices.
