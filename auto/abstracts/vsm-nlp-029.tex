In this paper we compare the performance of three approaches for estimating the la- tent weights of terms for scientific docu- ment summarization, given the document and a set of citing documents. The first ap- proach is a TF (term-frequency) vector space method utilizing a non-negative matrix fac- torization (NNMF) for dimensionality reduc- tion. The other two are language modeling ap- proaches used to predict the term distributions of human-generated summaries. The docu- ment is summarized by developing a language model that exploits the key sections of the document and a set of citing sentences from documents that cite the document of interest. The language model parameters may be set via a minimization of the Jensen-Shannon di- vergence. We use the OCCAMS algorithm (Optimal Combinatorial Covering Algorithm for Multi-document Summarization) to select a set of sentences that maximizes the top- ics coverage score. The results are evaluated using standard ROUGE metrics and the per- formance of the resulting methods achieves ROUGE scores exceeding those of the aver- age human summarizer.
