Abstractive dialogue summarization is challenging for several reasons: firstly, multiple speakers from different textual styles participate in dialogue, and secondly, informal dialogue structures (e.g., slang, colloquial representation). We constructed a syntax-aware model by leveraging linguistic information (i.e., POS tagging), which alleviates the above issues by inherently distinguishing sentences uttered from individual speakers. We employed multi-task learning of both syntax-aware information and dialogue summarization. Our approach is the first method to apply multi-task learning to the dialogue summarization task. Experiments on a SAMSum corpus (a large-scale dialogue summarization corpus) demonstrated that our method improved upon the vanilla model.
