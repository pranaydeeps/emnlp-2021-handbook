Although distributed    learning    has   increasingly gained  attention  in  terms  of  effectively  utilizing  local  devices for data privacy enhancement, recent studies show that publicly shared  gradients  in  the  training  process  can  reveal  the  private training data (gradient leakage) to a third-party. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on  the Transformer-based language models and propose a gradient  attack  algorithm,  TAG,  to  reconstruct  the  local  training  data. Experimental  results  on  Transformer,  TinyBERT4,  TinyBERT6 BERT\_BASE,  and  BERT\_LARGE using  GLUE  benchmark  show that compared  with  DLG,  TAG  works  well  on  more  weight distributions  in  reconstructing  training  data  and  achieves  1.5x recover rate and 2.5x ROUGE-2 over prior methods without the need of ground truth label.  TAG  can obtain up to 90\% data by attacking gradients in CoLA dataset. In addition, TAG is stronger than  previous  approaches  on  larger  models,  smaller  dictionary size,  and  smaller  input  length.  We  hope  the  proposed  TAG  will shed some light on the privacy leakage problem in Transformer-based  NLP  models.
