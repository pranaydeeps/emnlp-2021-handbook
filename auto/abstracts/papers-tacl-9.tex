We study learning named entity recognizers in the presence of missing entity annotations. We approach this setting as tagging with latent variables and propose a novel loss, the Expected Entity Ratio, to learn models in the presence of systematically missing tags. We show that our approach is both theoretically sound and empirically useful. Experimentally, we find that it outperforms previous state-of-the-art baselines both in accuracy and speed across a variety of languages, annotation scenarios, and amounts of labeled data. In particular, we find that it outperforms the previous state of the art by $+12.7$ F1 score in a challenging setting with only $1,000$ biased annotations. We also show that, when combined with our approach, a novel ``quick and sparse'' annotation scheme outperforms exhaustive annotation for modest annotation budgets.