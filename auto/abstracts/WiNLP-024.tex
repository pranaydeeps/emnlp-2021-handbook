Social biases such as queerphobia impact the development of NLP software meaning their output is often biased. In this paper we make the case for more nuanced tests for bias, and present a novel data set of 29,472 sentences to test for queerphobic bias in sentiment analysis tools. We use this data set to assess three commercially available models. None showed an overall bias against queer identities; however, we detected nuanced bias against minorities within the queer community, even in the two models which seem to have employed some superficial debiasing heuristic.
